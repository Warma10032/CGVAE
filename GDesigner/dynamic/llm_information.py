class Dyllm:
    def __init__(self):
        self.llm_info = {
            # OpenAI 系列
            "GPT-4.5": "GPT-4.5 是 OpenAI 最新一代模型，参数量估计接近 2 万亿，具备增强的推理能力、编程能力提升和更强的多语言支持能力，表现优异，适合多种高复杂度任务。",
            "GPT-4": "GPT-4 拥有 1.76 万亿参数，具备强大的推理与代码生成能力，并支持多模态输入，能够在复杂对话和多任务处理方面展现优秀表现。",
            "GPT-3.5": "GPT-3.5 拥有 1750 亿参数，擅长文本生成、代码补全和语言翻译，适用于日常应用和轻量级任务，但在复杂推理方面不如 GPT-4。",
            "GPT-3.5-turbo": "GPT-3.5-turbo 是 GPT-3.5 的精简版，参数量为 1750 亿，适用于轻量级任务和低资源环境。",
            "GPT-4o": "GPT-4o 是 OpenAI 推出的优化版模型，参数量为 1.76 万亿，具备更强的推理能力和多语言支持，适合复杂推理任务和多语言应用。",
            "GPT-4o-mini": "GPT-4o-mini 是 OpenAI 推出的优化版模型，参数量为 1.76 万亿，具备更强的推理能力和多语言支持，适合复杂推理任务和多语言应用。",

            # xAI 系列
            "Grok-3": "Grok-3 是 xAI 发布的大型语言模型，参数量为 3140 亿，支持实时知识更新、讽刺识别和 X 平台集成，适用于对实时性和语气理解有要求的场景。",

            # Meta LLaMA 系列
            "Llama-3.1-405B": "Llama-3.1-405B 是 Meta 开源的大模型，拥有 4050 亿参数，支持多语言和长上下文处理，适合多轮对话和跨语言任务，推理能力稳定可靠。",
            "Llama-3.3-70B-Instruct-Turbo": "Llama-3.3-70B-Instruct-Turbo 是 Meta 开源的大模型，拥有 7000 亿参数，支持多语言和长上下文处理，适合多轮对话和跨语言任务，推理能力稳定可靠。",
            "Llama-3.1-70B-Instruct-Turbo": "Llama-3.1-70B-Instruct-Turbo 是 Meta 开源的大模型，拥有 7000 亿参数，支持多语言和长上下文处理，适合多轮对话和跨语言任务，推理能力稳定可靠。",
            "Llama-2-13B": "Llama-2-13B 是 Meta 发布的中型开源模型，具有良好的文本生成和推理能力，适合部署在资源受限的服务器上进行本地推理。",

            # Google Gemini 系列
            "Gemini-1.5-Pro": "Gemini 1.5 Pro 是 Google 的多模态模型，具备超长上下文窗口（超过 200K tokens）和跨模态处理能力，适合视频、图像、文本混合任务。",
            "Gemini-2.0-flash": "Gemini 2.0 Flash 是 Google 推出的新一代模型，具有超长上下文窗口和多模态处理能力，支持视频、图像、文本混合任务。",
            
            # 阿里巴巴 Qwen 系列
            "QwQ-32B": "QwQ 是 Qwen 系列的推理模型。与传统的指令调优模型相比，QwQ 具备思考和推理能力，能够在下游任务中实现显著增强的性能，尤其是在解决困难问题方面。QwQ-32B 是中型推理模型，能够在与最先进的推理模型（如 DeepSeek-R1、o1-mini）的对比中取得有竞争力的性能。该模型采用 RoPE、SwiGLU、RMSNorm 和 Attention QKV bias 等技术，具有 64 层网络结构和 40 个 Q 注意力头（GQA 架构中 KV 为 8 个）",
            "Qwen2.5-72B": "Qwen2.5-72B-Instruct 是阿里云发布的最新大语言模型系列之一。该 72B 模型在编码和数学等领域具有显著改进的能力。它支持长达 128K tokens 的上下文。该模型还提供了多语言支持，覆盖超过 29 种语言，包括中文、英文等。模型在指令跟随、理解结构化数据以及生成结构化输出（尤其是 JSON）方面都有显著提升",
            "Qwen2.5-Coder-32B":"Qwen2.5-Coder-32B-Instruct 是基于 Qwen2.5 开发的代码特定大语言模型。该模型通过 5.5 万亿 tokens 的训练，在代码生成、代码推理和代码修复方面都取得了显著提升。它是当前最先进的开源代码语言模型，编码能力可与 GPT-4 相媲美。模型不仅增强了编码能力，还保持了在数学和通用能力方面的优势，并支持长文本处理",
            "Qwen1.5-1.8B": "Qwen1.5-1.8B 是轻量级模型，参数仅 18 亿，适合边缘设备和小型本地部署，虽然性能有限，但推理速度快、资源占用低。",

            # DeepSeek 系列
            "DeepSeek-V3": "DeepSeek-V3 是一款拥有 6710 亿参数的混合专家（MoE）语言模型，采用多头潜在注意力（MLA）和 DeepSeekMoE 架构，结合无辅助损失的负载平衡策略，优化推理和训练效率。通过在 14.8 万亿高质量tokens上预训练，并进行监督微调和强化学习，DeepSeek-V3 在性能上超越其他开源模型，接近领先闭源模型。",
            "DeepSeek-R1": "DeepSeek-R1 是一款强化学习（RL）驱动的推理模型，解决了模型中的重复性和可读性问题。在 RL 之前，DeepSeek-R1 引入了冷启动数据，进一步优化了推理性能。它在数学、代码和推理任务中与 OpenAI-o1 表现相当，并且通过精心设计的训练方法，提升了整体效果。",

            # 01.AI Yi 系列
            "Yi-34B": "Yi-34B 是 01.AI 推出的中文优化模型，参数量 340 亿，在中文理解和生成任务上表现出色，同时支持英文问答与多轮对话。",

            # 百川 Baichuan 系列
            "Baichuan2-13B": "Baichuan2-13B 是百川智能推出的中型模型，优化中文语义理解，适用于问答、摘要、搜索等通用任务，参数量为 130 亿。"
        }

    def get_llm_feature_information(self, llm_name):
        return self.llm_info.get(llm_name, "Model information not found.")
